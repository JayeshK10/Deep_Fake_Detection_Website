{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Fake.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZnnwIQQvtXED",
        "sa-t235l8eVv",
        "ZxrChNngViDH",
        "wjO41obLXJfR",
        "fWGAc31H_CPN",
        "5En8bvNKfNVY",
        "z-QC6mZRlUd0",
        "EJ2R-GxjGsw6",
        "XNzgrM0lagzQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h1RNILQsfU8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnnwIQQvtXED"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptvwJN5Ptahh"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "import dlib\n",
        "from skimage import io\n",
        "\n",
        "from numpy import asarray\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "sa-t235l8eVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/Deep_Fake/train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Deep_Fake/test.csv')\n",
        "df_test_all = pd.read_csv('/content/drive/MyDrive/Deep_Fake/test_all.csv')\n",
        "\n",
        "# train_img_df = pd.read_csv('/content/drive/MyDrive/Deep_Fake/train_img.csv')\n",
        "# test_img_df = pd.read_csv('/content/drive/MyDrive/Deep_Fake/test_img.csv')\n",
        "\n",
        "with open('/content/drive/MyDrive/Deep_Fake/data_all.pkl', 'rb') as f:\n",
        "    dic_all = pickle.load(f)"
      ],
      "metadata": {
        "id": "O0DYjroB8ghA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxrChNngViDH"
      },
      "source": [
        "# Img exteration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgPath=[]\n",
        "resize= 1\n",
        "frame_count_real = []\n",
        "frame_count_fake = []\n",
        "\n",
        "\n",
        "# Function to extract frames\n",
        "def FrameCapture(vid_path , vid_name , n_frames):\n",
        "    imgPath = []\n",
        "    v_cap = cv2.VideoCapture(vid_path)\n",
        "    v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    # Pick 'n_frames' evenly spaced frames to sample\n",
        "    sample = np.linspace(0, v_len - 1, n_frames).astype(int) #Index in sample array are the frames which will be extracted.\n",
        "\n",
        "    # Loop through frames\n",
        "    \n",
        "    frames = []\n",
        "    for j in range(v_len):\n",
        "        success = v_cap.grab()\n",
        "        if j in sample:\n",
        "            # Load frame\n",
        "            success, frame = v_cap.read()\n",
        "            if not success:          \n",
        "                continue\n",
        "            \n",
        "            frame = Image.fromarray(frame)\n",
        "            \n",
        "            # Resize frame to desired size\n",
        "            if resize is not None:\n",
        "                frame = frame.resize([int(d * resize) for d in frame.size])\n",
        "                frame = np.asarray(frame)\n",
        "            frames.append(frame)\n",
        "    \n",
        "    # Used as counter variable\n",
        "  \n",
        "    # checks whether frames were extracted\n",
        "    currentCount = 0\n",
        "    success = 1\n",
        "  \n",
        "    while success and currentCount<min(n_frames,len(frames)):\n",
        "        cv2.imwrite(\"/content/drive/MyDrive/Deep_Fake/Frames/Train/\" + vid_name + '_' + str(currentCount) + '.png' ,frames[int(currentCount)])\n",
        "        imgPath.append(\"/content/drive/MyDrive/Deep_Fake/Frames/Train/\" + vid_name + '_' + str(currentCount))\n",
        "        currentCount += 1\n",
        "\n",
        "    return imgPath"
      ],
      "metadata": {
        "id": "MV5bZRBm6KEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_frames = 11\n",
        "resize= 1\n",
        "\n",
        "def captureFrame(df):\n",
        "  imgPath = []\n",
        "  video_label = []\n",
        "  vid_name = []\n",
        "\n",
        "  for row in df.iterrows():\n",
        "      video_name = row[1][1]\n",
        "      video_path = row[1][-2]\n",
        "      category = row[1][-1]\n",
        "\n",
        "      img_path = FrameCapture(video_path , video_name , n_frames)\n",
        "      label = [category] * len(img_path)\n",
        "      vd_name = [video_name] * len(img_path)\n",
        "\n",
        "      imgPath = imgPath + img_path\n",
        "      video_label = video_label + label\n",
        "      vid_name = vid_name + vd_name\n",
        "\n",
        "  return imgPath , video_label"
      ],
      "metadata": {
        "id": "s84UD8zKCb0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_path, train_img_label = captureFrame(df_train)"
      ],
      "metadata": {
        "id": "NVEA3zx_IGs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_path = [s + '.png' for s in train_img_path]\n",
        "test_img_path = [s + '.png' for s in test_img_path]"
      ],
      "metadata": {
        "id": "q5JkoOMCghqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_train = {'train_img_path': train_img_path , 'train_img_label':train_img_label}\n",
        "train_img_df = pd.DataFrame(dic_train)\n",
        "train_img_df.to_csv('/content/drive/MyDrive/Deep_Fake/train_img.csv')"
      ],
      "metadata": {
        "id": "_1RsO2huRiIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_img_path, test_img_label = captureFrame(df_test)"
      ],
      "metadata": {
        "id": "oB4mcPEfRiVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_test = {'test_img_path': test_img_path , 'test_img_label':test_img_label}\n",
        "test_img_df = pd.DataFrame(dic_test)\n",
        "test_img_df.to_csv('/content/drive/MyDrive/Deep_Fake/test_img.csv')"
      ],
      "metadata": {
        "id": "ElF0XnXHXGRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Face Detection"
      ],
      "metadata": {
        "id": "wjO41obLXJfR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGiz09A4P66M"
      },
      "source": [
        "def detect_faces(image):\n",
        "    # Create a face detector\n",
        "    face_detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "    # Run detector and get bounding boxes of the faces on image.\n",
        "    detected_faces = face_detector(image, 1)\n",
        "    face_frames = [(x.left(), x.top(), x.right(), x.bottom()) for x in detected_faces]\n",
        "\n",
        "    return face_frames\n",
        "\n",
        "\n",
        "def faceDetect_DLIB(imgPath, imgLabel):\n",
        "  processedImagePath_dlib = []\n",
        "  processedImageLab_dlib = []\n",
        "\n",
        "  for i in range(len(imgPath)):\n",
        "      # Load image\n",
        "      img_path = imgPath[i]\n",
        "      image = io.imread(img_path)\n",
        "\n",
        "      label = imgLabel[i]\n",
        "\n",
        "      # Detect faces\n",
        "      detected_faces = detect_faces(image)\n",
        "\n",
        "      # Crop faces and plot\n",
        "      for n, face_rect in enumerate(detected_faces):\n",
        "          face = Image.fromarray(image).crop(face_rect)\n",
        "          final_face = asarray(face)\n",
        "          resize_face = cv2.resize(final_face,(64, 64), interpolation=cv2.INTER_CUBIC)\n",
        "          finalImg = cv2.cvtColor(resize_face, cv2.COLOR_BGR2RGB)\n",
        "          #cv2.imwrite(\"/content/drive/MyDrive/Deep_Fake/Frames/FD_test_/\" + str(i+1) + '.png' , finalImg)\n",
        "          processedImagePath_dlib.append(\"/content/drive/MyDrive/Deep_Fake/Frames/FD_train/\" + str(i+1) + '.png' )\n",
        "          processedImageLab_dlib.append(label)\n",
        "\n",
        "  return processedImagePath_dlib , processedImageLab_dlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiaWCUfB3mff"
      },
      "source": [
        "fd_train_img_path , fd_train_img_label = faceDetect_DLIB(dic_all['train_img_path'] , dic_all['train_img_label'])\n",
        "dic_all['fd_train_img_label'] = fd_train_img_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fd_test_img_path , fd_test_img_label = faceDetect_DLIB(dic_all['test_img_path'] , dic_all['test_img_label'])\n",
        "dic_all['fd_test_img_label'] = fd_test_img_label"
      ],
      "metadata": {
        "id": "JCFx4veyb1Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Deep_Fake/data_all.pkl', 'wb') as f:\n",
        "    pickle.dump(dic_all, f)"
      ],
      "metadata": {
        "id": "MvPw2fPNi4F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWGAc31H_CPN"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQc56jZ8_EnL"
      },
      "source": [
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from skimage.util import random_noise\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def noisy(noise_typ,image):\n",
        "  if noise_typ == \"gauss\":\n",
        "    mean = 0\n",
        "    var = 100\n",
        "    sigma = var ** 0.5\n",
        "    row, col, _ = image.shape\n",
        "    gaussian = np.random.normal(mean, sigma, (row, col)) \n",
        "\n",
        "    noisy = np.zeros(image.shape, np.float32)\n",
        "\n",
        "    if len(image.shape) == 2:\n",
        "        noisy = image + gaussian\n",
        "    else:\n",
        "        noisy[:, :, 0] = image[:, :, 0] + gaussian\n",
        "        noisy[:, :, 1] = image[:, :, 1] + gaussian\n",
        "        noisy[:, :, 2] = image[:, :, 2] + gaussian\n",
        "\n",
        "    cv2.normalize(noisy, noisy, 0, 255, cv2.NORM_MINMAX, dtype=-1)\n",
        "    noisy = noisy.astype(np.uint8)\n",
        "    return noisy\n",
        "  elif noise_typ == \"s&p\":\n",
        "    row,col,ch = image.shape\n",
        "    s_vs_p = 0.5\n",
        "    amount = 0.05\n",
        "    out = np.copy(image)\n",
        "    # Salt mode\n",
        "    num_salt = np.ceil(amount * image.size * s_vs_p)\n",
        "    coords = [np.random.randint(0, i - 1, int(num_salt)) for i in image.shape]\n",
        "    out[coords] = 1\n",
        "\n",
        "    # Pepper mode\n",
        "    num_pepper = np.ceil(amount* image.size * (1. - s_vs_p))\n",
        "    coords = [np.random.randint(0, i - 1, int(num_pepper)) for i in image.shape]\n",
        "    out[coords] = 0\n",
        "    return out\n",
        "\n",
        "  elif noise_typ ==\"speckle\":\n",
        "    row,col,ch = image.shape\n",
        "    gauss = np.random.randn(row,col,ch)\n",
        "    gauss = gauss.reshape(row,col,ch)\n",
        "    noisy = image + image * gauss\n",
        "    return noisy\n",
        "\n",
        "def create_noisy_img(face_detector_arr):\n",
        "    arr=[]\n",
        "    for i in range(len(face_detector_arr)):\n",
        "        img=cv2.imread(face_detector_arr[i])\n",
        "        \n",
        "        noisy_img = noisy(\"gauss\", img)\n",
        "        noisy_img = cv2.resize(noisy_img, (64, 64))\n",
        "        \n",
        "        cv2.imwrite(\"/content/drive/MyDrive/Deep_Fake/Frames/noise/Test/\" + str(i+1) + '.png' ,noisy_img)\n",
        "        arr.append(\"/content/drive/MyDrive/Deep_Fake/Frames/noise/Test/\" + str(i+1) + '.png' )\n",
        "    return arr"
      ],
      "metadata": {
        "id": "XObX8-VVMwmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_all.keys()"
      ],
      "metadata": {
        "id": "sspKR5nqMxQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c320a743-e5b1-4bea-e195-67d1fcd3772d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['train_img_path', 'train_img_label', 'test_img_path', 'test_img_label', 'fd_train_img_path', 'fd_test_img_path'])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fd_train_img_path_noise = create_noisy_img(dic_all['fd_train_img_path'])"
      ],
      "metadata": {
        "id": "DC2ezAr0uWhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_all['fd_train_img_path_noise'] = fd_train_img_path_noise"
      ],
      "metadata": {
        "id": "cSkb2t-vylum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fd_test_img_path_noise = create_noisy_img(dic_all['fd_test_img_path'])"
      ],
      "metadata": {
        "id": "hcCNfXbruWk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_all['fd_test_img_path_noise'] = fd_test_img_path_noise"
      ],
      "metadata": {
        "id": "CD9DAqJhzcHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Deep_Fake/data_all.pkl', 'wb') as f:\n",
        "    pickle.dump(dic_all, f)"
      ],
      "metadata": {
        "id": "ZV0y7JzLzom5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train CNN"
      ],
      "metadata": {
        "id": "5En8bvNKfNVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n"
      ],
      "metadata": {
        "id": "LJNHBMUG-dqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def img_to_tensor(img_path):\n",
        "    img = Image.open(img_path)\n",
        "    img_resize = img.convert('RGB').resize((128, 128))\n",
        "    tensor = np.round(np.array(img_resize,dtype=np.float32))\n",
        "    data_tensor = tf.convert_to_tensor(tensor)\n",
        "    return data_tensor"
      ],
      "metadata": {
        "id": "dCVUntOjfpPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data(arr):\n",
        "    re_arr = []\n",
        "    for i in arr:\n",
        "        re_arr.append(img_to_tensor(i))\n",
        "    return re_arr\n",
        "\n",
        "dic_all['X_train_tensor'] = create_data(dic_all['fd_train_img_path_noise'])\n",
        "dic_all['X_test_tensor'] = create_data(dic_all['fd_test_img_path_noise'])"
      ],
      "metadata": {
        "id": "_mMGu1gdgNg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_label(arr):\n",
        "    x = []\n",
        "    for i in range(len(arr)):\n",
        "        if(arr[i]==1):\n",
        "            x.append([0,1])\n",
        "        else:\n",
        "            x.append([1,0])\n",
        "\n",
        "    return x\n",
        "\n",
        "dic_all['Y_train'] = create_label(dic_all['fd_train_img_label'])\n",
        "dic_all['Y_test'] = create_label(dic_all['fd_test_img_label'])"
      ],
      "metadata": {
        "id": "SOMtUGM8_NGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Deep_Fake/data_all.pkl', 'wb') as f:\n",
        "    pickle.dump(dic_all, f)"
      ],
      "metadata": {
        "id": "qz0IttVt9G92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH7mDYMz_HkO"
      },
      "source": [
        "# Parameters\n",
        "BATCH_SIZE = 50\n",
        "max_epochs = 5\n",
        "\n",
        "# Datasets\n",
        "\n",
        "X_train = dic_all['X_train_tensor']\n",
        "X_test = dic_all['X_test_tensor']\n",
        "Y_train = dic_all['Y_train']\n",
        "Y_test = dic_all['Y_test']\n",
        "\n",
        "# Generators\n",
        "training_generator = tf.data.Dataset.from_tensor_slices((X_train , Y_train))\n",
        "test_generator = tf.data.Dataset.from_tensor_slices((X_test , Y_test))\n",
        "\n",
        "#Augment and shuffle\n",
        "def augment(points, label):\n",
        "    # jitter points\n",
        "    points += tf.random.uniform(points.shape, -0.005, 0.005, dtype=tf.float32)\n",
        "    # shuffle points\n",
        "    points = tf.random.shuffle(points)\n",
        "    return points, label\n",
        "\n",
        "# train_dataset = training_generator.shuffle(len(training_set[0])).map(augment).batch(BATCH_SIZE)\n",
        "# test_dataset = test_generator.shuffle(len(test_set[0])).batch(BATCH_SIZE)\n",
        "# val_dataset = validation_generator.shuffle(len(validation_set[0])).batch(BATCH_SIZE)\n",
        "\n",
        "train_dataset = training_generator.batch(BATCH_SIZE)\n",
        "test_dataset = test_generator.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjLnfNSJczcL",
        "outputId": "75a20817-176c-4532-fe14-a6abc0bf6f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((None, 128, 128, 3), (None, 2)), types: (tf.float32, tf.int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense"
      ],
      "metadata": {
        "id": "qnXM_kdu5zLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    model= Sequential()\n",
        "    model.add(Conv2D(filters=16, kernel_size=2, activation='tanh', input_shape=(128,128,3)))\n",
        "    model.add(MaxPooling2D(pool_size=2))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(filters=32,kernel_size = 2,activation='tanh'))\n",
        "    model.add(MaxPooling2D(pool_size=2))\n",
        "    model.add(Dropout(0.2))\n",
        "    # model.add(Conv2D(filters=64,kernel_size = 2,activation='tanh'))\n",
        "    # model.add(MaxPooling2D(pool_size=2))\n",
        "    # model.add(Conv2D(filters=128,kernel_size = 2,activation='tanh'))\n",
        "    # model.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # model.add(Dense(50,activation='relu'))\n",
        "    model.add(Dense(10,activation='relu'))\n",
        "    model.add(Dense(2,activation = 'softmax'))\n",
        "        \n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='Adam',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "qBxm17kE53ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ihYECVza56EV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "543e2fe7-c252-4be1-a17c-0d752e54a178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 127, 127, 16)      208       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 63, 63, 16)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 63, 63, 16)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 62, 62, 32)        2080      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 31, 31, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 31, 31, 32)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 30752)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                307530    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 22        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 309,840\n",
            "Trainable params: 309,840\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset , epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9bZ6HQGCuTs",
        "outputId": "89ec0d77-90c9-49b8-f704-83941a4873c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "95/95 [==============================] - 13s 31ms/step - loss: 0.7402 - accuracy: 0.6997\n",
            "Epoch 2/3\n",
            "95/95 [==============================] - 3s 29ms/step - loss: 0.6753 - accuracy: 0.7055\n",
            "Epoch 3/3\n",
            "95/95 [==============================] - 3s 28ms/step - loss: 0.6640 - accuracy: 0.7055\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0ab86f2b50>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "uac_-sR15-hk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2f2bdc1-999b-4f32-e5bd-036a67ac1c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 1s 25ms/step - loss: 0.6531 - accuracy: 0.7271\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6531263589859009, 0.7270875573158264]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/Deep_Fake/model_cnn.ckpt\""
      ],
      "metadata": {
        "id": "MI8rj_V26A4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.save_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "y_3F1jqi6A72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "mJ-EbJUW6A_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f99107-b872-4373-b191-5a5e6ee6dc67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb34ea418d0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u92kgGAlIyu",
        "outputId": "0a64faef-e9d5-455f-833b-878ea124aa8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 9s 24ms/step - loss: 0.6531 - accuracy: 0.7271\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6531263589859009, 0.7270875573158264]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "z-QC6mZRlUd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "j = 0\n",
        "arr_ = []\n",
        "for i in test_dataset:\n",
        "    arr = i[0][j]\n",
        "    arr_.append(arr)\n",
        "    j+=1\n",
        "    if j > 10:\n",
        "        break"
      ],
      "metadata": {
        "id": "5xxxHsyxlW4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr_ = tf.expand_dims(arr, axis=0)\n",
        "print(arr_.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNz3kVTklW70",
        "outputId": "7e325c58-3247-4add-88dd-6f6a64fad205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 128, 128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr__ = tf.stack(arr_)\n",
        "print(arr__.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqhWjDqVoUCS",
        "outputId": "89938bc6-7192-4a1d-bec9-9b5aae308f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 128, 128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = model.predict(arr__)"
      ],
      "metadata": {
        "id": "JuGdSRpDncDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oS-pmHA3nzMy",
        "outputId": "f4df5ad7-cfd6-4052-c6e0-57ab1a74326a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5920801 , 0.40791988],\n",
              "       [0.5920801 , 0.40791988],\n",
              "       [0.5920801 , 0.40791988],\n",
              "       [0.5920801 , 0.40791988],\n",
              "       [0.5920801 , 0.40791988],\n",
              "       [0.5920801 , 0.40791988],\n",
              "       [0.5920801 , 0.40791988],\n",
              "       [0.5920801 , 0.40791988],\n",
              "       [0.5920801 , 0.40791988],\n",
              "       [0.5920801 , 0.40791988]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_ans(arr__ , model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgStY5OQn1dO",
        "outputId": "a7c2d0c1-a7f4-42d1-b15b-363c915b29d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "The video is FAKE!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "EJ2R-GxjGsw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = dic_all.keys()\n",
        "for i in a:\n",
        "    print(i , len(dic_all[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fW252iJGu4H",
        "outputId": "cf69777c-9827-4e07-a85e-9ee0ad731e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_img_path 4937\n",
            "train_img_label 4937\n",
            "test_img_path 536\n",
            "test_img_label 536\n",
            "fd_train_img_path 4716\n",
            "fd_test_img_path 491\n",
            "fd_train_img_path_noise 4716\n",
            "fd_test_img_path_noise 491\n",
            "fd_test_img_label 491\n",
            "fd_train_img_label 4716\n",
            "X_train 4716\n",
            "X_test 491\n",
            "X_train_tensor 4716\n",
            "X_test_tensor 491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline\n"
      ],
      "metadata": {
        "id": "XNzgrM0lagzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import asarray\n",
        "\n",
        "import cv2 \n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "import dlib\n",
        "from skimage import io\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense"
      ],
      "metadata": {
        "id": "gVWnB8BBUCUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_detect_2(vid_path , model):\n",
        "    imgPath = []\n",
        "    n_frames = 11\n",
        "    resize= 1\n",
        "    v_cap = cv2.VideoCapture(vid_path)\n",
        "    v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    sample = np.linspace(0, v_len - 1, n_frames).astype(int) \n",
        "\n",
        "    frames = []\n",
        "    for j in range(v_len):\n",
        "        success = v_cap.grab()\n",
        "        if j in sample:\n",
        "            success, frame = v_cap.read()\n",
        "            if not success:          \n",
        "                continue\n",
        "            \n",
        "            frame = Image.fromarray(frame)\n",
        "            \n",
        "            if resize is not None:\n",
        "                frame = frame.resize([int(d * resize) for d in frame.size])\n",
        "                frame = np.asarray(frame)\n",
        "            frames.append(frame)\n",
        "    \n",
        "    currentCount = 0\n",
        "    success = 1\n",
        "  \n",
        "    while success and currentCount<min(n_frames,len(frames)):\n",
        "        cv2.imwrite(\"/content/img_\" + str(currentCount) + '.png' ,frames[int(currentCount)])\n",
        "        imgPath.append(\"/content/img_\" + str(currentCount) + '.png')\n",
        "        currentCount += 1\n",
        "\n",
        "    face_tensor = []\n",
        "\n",
        "    for i in range(len(imgPath)):\n",
        "        image = io.imread(imgPath[i])\n",
        "\n",
        "        face_detector = dlib.get_frontal_face_detector()\n",
        "        detected_faces = face_detector(image, 1)\n",
        "        face_frames = [(x.left(), x.top(), x.right(), x.bottom()) for x in detected_faces]\n",
        "        \n",
        "        for n, face_rect in enumerate(face_frames):\n",
        "            face = Image.fromarray(image).crop(face_rect)\n",
        "            final_face = asarray(face)\n",
        "            resize_face = cv2.resize(final_face,(128, 128), interpolation=cv2.INTER_CUBIC)\n",
        "            finalImg = cv2.cvtColor(resize_face, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            tensor = np.round(np.array(finalImg,dtype=np.float32))\n",
        "            data_tensor = tf.convert_to_tensor(tensor)\n",
        "            data_tensor_ = tf.expand_dims(data_tensor, axis=0)\n",
        "            face_tensor.append(data_tensor_)\n",
        "\n",
        "    count = 0\n",
        "    for i in range(len(face_tensor)):\n",
        "        p =  model.predict(face_tensor[i])\n",
        "        pr = 0 if p[0][0] > p[0][1] else 1\n",
        "        count += pr\n",
        "    if count < 6:\n",
        "        print('The video is FAKE!!')\n",
        "    else :\n",
        "        print('The video is REAL!!')"
      ],
      "metadata": {
        "id": "_F9lzvQrP0fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    model= Sequential()\n",
        "    model.add(Conv2D(filters=16, kernel_size=2, activation='tanh', input_shape=(128,128,3)))\n",
        "    model.add(MaxPooling2D(pool_size=2))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(filters=32,kernel_size = 2,activation='tanh'))\n",
        "    model.add(MaxPooling2D(pool_size=2))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(10,activation='relu'))\n",
        "    model.add(Dense(2,activation = 'softmax'))\n",
        "        \n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='Adam',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "s2ukbzwbTvQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "checkpoint_path = \"/content/drive/MyDrive/Deep_Fake/model_cnn.ckpt\"\n",
        "model.load_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "1nB_AZq9T4fi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8a3b61-32d2-45c2-c2c7-2588bbbeb5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f77c73ac110>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(model,open(\"/content/drive/MyDrive/Deep_Fake/new_model.pkl\",'wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG9PJ_sV3318",
        "outputId": "b098dd47-30bb-4408-e80c-2ae32f5b1c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://a0c24bf2-831b-46f4-be05-fe2e4ffcf93b/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_json = model.to_json()\n",
        "with open('/content/model.json' , 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights('/content/model.h5')\n",
        "model.save('/content/modelf.h5')"
      ],
      "metadata": {
        "id": "81JmHko1NPL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = df_test.loc[0][3]"
      ],
      "metadata": {
        "id": "_cBpH9G3qUcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_f = pickle.load(open('/content/drive/MyDrive/Deep_Fake/new_model.pkl','rb'))"
      ],
      "metadata": {
        "id": "6FVm3lQzqYce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_detect_2(path , model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vxOU5WQSlgF",
        "outputId": "295bf550-973c-4cff-fb21-97ca22290abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The video is REAL!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_detect_2(path , model_f)"
      ],
      "metadata": {
        "id": "TWKYEcpRP0oK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a49cb57-812f-4048-8db5-cb8be6406e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The video is REAL!!\n"
          ]
        }
      ]
    }
  ]
}